{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Part C: Trying alternative classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a skeleton for trying alternative classifiers on the basketball dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define, as done in Practicum 6, a data loading in a way to obtain the attributes set and class labels for each the training and the test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ATTRS = [\"LOCATION\", \"W\", \"FINAL_MARGIN\", \"SHOT_NUMBER\", \"PERIOD\", \"GAME_CLOCK\", \"SHOT_CLOCK\", \n",
    "         \"DRIBBLES\", \"TOUCH_TIME\", \"SHOT_DIST\", \"PTS_TYPE\", \"CLOSE_DEF_DIST\", \"SHOT_RESULT\"]\n",
    "\n",
    "ATTRS_WO_CLASS = 12\n",
    "\n",
    "\"\"\" This method loads data for training and evaluating the classifiers \"\"\"\n",
    "def load_train_data(filename):\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    with open(filename, 'rt') as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        i = 0\n",
    "        for row in csvreader:\n",
    "            if len(row) == ATTRS_WO_CLASS + 1:\n",
    "                i += 1\n",
    "                instance = [row[i] for i in range(ATTRS_WO_CLASS)]  # first ATTRS_WO_CLASS values are attributes\n",
    "                label = row[ATTRS_WO_CLASS]  # (ATTRS_WO_CLASS + 1)th value is the class label\n",
    "                if i % 2 == 0:  # test instance\n",
    "                    test_x.append(instance)\n",
    "                    test_y.append(label)\n",
    "                else:  # train instance\n",
    "                    train_x.append(instance)\n",
    "                    train_y.append(label)\n",
    "                    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\"\"\" This method loads the data that the trained classifiers will test \"\"\"\n",
    "def load_test_data(filename):\n",
    "    test_data = []  # instances to be tested\n",
    "    with open(filename, 'rt') as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        i = 0\n",
    "        for row in csvreader:\n",
    "            if len(row) == ATTRS_WO_CLASS:\n",
    "                i += 1\n",
    "                instance = [row[i] for i in range(ATTRS_WO_CLASS)]  \n",
    "                test_data.append(instance)  \n",
    "                    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can use it to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_FILE = \"data/classifier_basketball.pred.csv\"\n",
    "TRAINING_FILE = \"data/basketball.train.csv\"\n",
    "TESTING_FILE = \"data/basketball.test.csv\"\n",
    "\n",
    "train_x, train_y, test_x, test_y = load_train_data(TRAINING_FILE) \n",
    "test_data = load_test_data(TESTING_FILE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predictions evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(predictions, true_labels):\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == true_labels[i]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            incorrect += 1\n",
    "\n",
    "    print(\"\\tAccuracy:   \", correct / len(predictions))\n",
    "    print(\"\\tError rate: \", incorrect / len(predictions))\n",
    "    \n",
    "    return correct / len(predictions)  # return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn needs that all the attribute values to be numeric. This is, we need to binarize all the non-numeric attribute values, to obtain vectors: records having only numbers. The `DictVectorizer` class provided by scikit-learn allows to do this easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mind that each `train_x` and `test_x` are a list of lists.\n",
    "\n",
    "We just need to obtain from each a list of dictionaries (as done in previous practica where each record was a dictionary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do similarly for vectorizing test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "\"\"\" This method vectorizes whatever needs to be vectorized \"\"\"\n",
    "def vectorize(train_test):    \n",
    "    dicts_train_test = []\n",
    "    for x in train_test:\n",
    "        d = {}\n",
    "        for i, attr in enumerate(ATTRS):\n",
    "            if i < len(ATTRS) - 1: # we removed class from test elems\n",
    "                val = x[i]\n",
    "                # save as floats the values for the already-numeric attributes from dataset \n",
    "                # keep the rest as the strings they are\n",
    "                if i not in [0, 1, 4, 10]:  # indices for \"LOCATION\", \"W\", \"PERIOD\", \"PTS_TYPE\" attributes\n",
    "                    val = float(val)\n",
    "                d[attr] = val\n",
    "        dicts_train_test.append(d)\n",
    "\n",
    "    # Finally, the fit_transform method of the vectorizer binarizes \n",
    "    # the non-numeric attributes in the list of dictionaries, and returns the vector we need.\n",
    "    vectorizer_train:test = DictVectorizer()\n",
    "    vec_train_test = vectorizer_train.fit_transform(dicts_train_test).toarray()\n",
    "    \n",
    "    return vec_train_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having `evaluate` defined somewhere, we are ready to learn and apply the model, similarly to Task 3 of Practicum 6. But here, we use the vectors recently obtained for the input sets. E.g., for Naive Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "\tAccuracy:    0.5359825539132542\n",
      "\tError rate:  0.46401744608674583\n",
      "AdaBoost_stock\n",
      "\tAccuracy:    0.5951592493875023\n",
      "\tError rate:  0.40484075061249764\n",
      "AdaBoost_23_SAMME\n",
      "\tAccuracy:    0.5991169264733597\n",
      "\tError rate:  0.40088307352664027\n",
      "AdaBoost_100_SAMME_12estimator\n",
      "\tAccuracy:    0.5950246345206365\n",
      "\tError rate:  0.4049753654793635\n",
      "AdaBoost_100_SAMMER_12estimator\n",
      "\tAccuracy:    0.5922246452898258\n",
      "\tError rate:  0.4077753547101742\n",
      "AdaBoost_23_SAMME_12estimator\n",
      "\tAccuracy:    0.5950784804673828\n",
      "\tError rate:  0.4049215195326172\n",
      "AdaBoost_123_SAMMER_12estimator\n",
      "\tAccuracy:    0.5935169480117384\n",
      "\tError rate:  0.40648305198826157\n",
      "AdaBoost_23\n",
      "\tAccuracy:    0.5989553886331206\n",
      "\tError rate:  0.40104461136687936\n"
     ]
    }
   ],
   "source": [
    "classifiers2 = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Nearest Neighbors\": KNeighborsClassifier(n_neighbors=750),\n",
    "    \"Naive Bayes (Gaussian)\": GaussianNB(priors=[0.40, 0.60]),  # priors = prior probabilities of the two classes\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=42, max_features=\"auto\"), \n",
    "    \"AdaBoost\": AdaBoostClassifier(learning_rate=0.23),\n",
    "    \"Nearest Centroid\": NearestCentroid(metric='euclidean', shrink_threshold=0.2),\n",
    "    \"RidgeClassifier_auto\" : RidgeClassifier(alpha=1.0, fit_intercept=True, tol=0.001, class_weight=None, \n",
    "                                        solver='auto', random_state=None),\n",
    "    # \"SVC\": SVC(C=1.0, kernel='poly', degree=3, coef0=0.0, \n",
    "    #          tol=0.001, class_weight='balanced', max_iter=1000, decision_function_shape='ovr'),\n",
    "    # \"Support Vector Classification\": SVC(kernel=\"rbf\", decision_function_shape=\"ovo\", max_iter=10000)\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"AdaBoost_stock\": AdaBoostClassifier(base_estimator=None, n_estimators=50, \n",
    "                                         learning_rate=1.0, algorithm='SAMME.R', random_state=None),\n",
    "    \"AdaBoost_23_SAMME\": AdaBoostClassifier(base_estimator=None, n_estimators=50, \n",
    "                                         learning_rate=0.23, algorithm='SAMME', random_state=None),\n",
    "    \"AdaBoost_100_SAMME_12estimator\": AdaBoostClassifier(base_estimator=None, n_estimators=12, \n",
    "                                         learning_rate=1.0, algorithm='SAMME', random_state=None),\n",
    "    \"AdaBoost_23_SAMME_12estimator\": AdaBoostClassifier(base_estimator=None, n_estimators=12, \n",
    "                                         learning_rate=0.23, algorithm='SAMME', random_state=None),\n",
    "    \n",
    "    \n",
    "    \"AdaBoost_23\": AdaBoostClassifier(learning_rate=0.23),\n",
    "    \n",
    "}\n",
    "\n",
    "vec_test_x = vectorize(test_x)\n",
    "vec_train_x = vectorize(train_x)\n",
    "\n",
    "best_accuracy = 0\n",
    "best_clf_name = \"\"\n",
    "best_clf = None\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(name)\n",
    "    clf.fit(vec_train_x, train_y)\n",
    "    predictions = clf.predict(vec_test_x)\n",
    "    accuracy = evaluate(predictions, test_y) \n",
    "    if accuracy > best_accuracy: \n",
    "        best_accuracy = accuracy\n",
    "        best_clf_name = name\n",
    "        best_clf = clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the best classifier, use it on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from AdaBoost written to 'data/classifier_basketball.pred.csv'\n"
     ]
    }
   ],
   "source": [
    "if best_accuracy > 0.57887:  # if the score is higher than what we got with the decision tree from part B\n",
    "    vec_test_data = vectorize(test_data)  # ready the actual test data\n",
    "    final_preds = best_clf.predict(vec_test_data)  # use the best classifier we found on the test data\n",
    "    \n",
    "    with open(OUTPUT_FILE, 'w') as output:\n",
    "        output.write(\"Id,Target\")\n",
    "        for id, pred in enumerate(final_preds):\n",
    "            output.write(\"\\n%s,%s\" % (id + 1, pred))\n",
    "            \n",
    "    print(\"Results from %s written to '%s'\" % (best_clf_name, OUTPUT_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
