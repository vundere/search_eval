{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Implement a learning-to-rank method with the following minimum requirements: \n",
    "        - Consider document-query matching in minimum 3 fields (title, content and anchors) \n",
    "          and at least two different retrieval models (e.g., BM25 and LM). That is, 6 document-query features minimum.\n",
    "\n",
    "Perform baseline (BM25) retrieval on a separate anchor text index. (THIS IS DONE IN 1_baseline.ipynb)\n",
    "        - The anchor text index (called clueweb12b_anchors) can be accessed the same way as the regular document index. \n",
    "        - Note that the anchor text index covers the entire ClueWeb collection, not just the Category B subset. \n",
    "          I.e., you need to ignore documents that are not present in the regular index.\n",
    "\n",
    "Test your model using 5-fold cross-validation on the given training data (queries and relevance judments, i.e., data/queries.txt and data/qrels.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "Load queries and qrels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "\n",
    "\n",
    "API = \"http://gustav1.ux.uis.no:5002\"\n",
    "\n",
    "QUERY_FILE = \"data/queries.txt\"\n",
    "QRELS_FILE = \"data/qrels.csv\"\n",
    "\n",
    "FEATURES_FILE = \"data/features.txt\"\n",
    "OUTPUT_FILE = \"data/ltr.txt\"  # output the ranking\n",
    "\n",
    "# load queries\n",
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(QUERY_FILE, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries\n",
    "        \n",
    "# load given ground truth\n",
    "def load_qrels(qrels_file):\n",
    "    gtruth = {}\n",
    "    with open(QRELS_FILE, 'r') as qr:\n",
    "        for line in qr.readlines():\n",
    "            if line.startswith('QueryId'):\n",
    "                continue\n",
    "            qid, did, rel = line.strip().split(',')\n",
    "            if qid not in gtruth:\n",
    "                gtruth[qid] = {}\n",
    "            gtruth[qid][did] = int(rel)\n",
    "    return gtruth\n",
    "\n",
    "queries = load_queries(QUERY_FILE)\n",
    "gtruth = load_qrels(QRELS_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(indexname, query, field, size=10):\n",
    "    url = \"/\".join([API, indexname, \"_search\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"q\": query, \"df\": field, \"size\": size})\n",
    "    response = requests.get(url).text\n",
    "    return json.loads(response)\n",
    "\n",
    "\n",
    "def termvectors(indexname, docid, term_statistics=\"true\"): \n",
    "    url = \"/\".join([API, indexname, docid, \"_termvectors\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"term_statistics\": term_statistics})\n",
    "    response = requests.get(url).text\n",
    "    return json.loads(response)\n",
    "\n",
    "def exists(indexname, docid): \n",
    "    url = \"/\".join([API, indexname, docid, \"_exists\"])\n",
    "    response = requests.get(url).text\n",
    "    return json.loads(response)\n",
    "\n",
    "def analyze_query(indexname, query):\n",
    "    url = \"/\".join([API, indexname, \"_analyze\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"text\": query})\n",
    "    response = json.loads(requests.get(url).text)\n",
    "    tokens = response[\"tokens\"]\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x[\"position\"]):\n",
    "        query_terms.append(t[\"token\"])\n",
    "    return query_terms\n",
    "\n",
    "# print(termvectors(\"clueweb12b\", \"clueweb12-0000tw-07-01629\").items())\n",
    "# print(analyze_query(\"clueweb12b\", \"raspberry pi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "Collecting feature values in the features dict. It has the structure features[qid][docid][fid] = value, where fid is a feature ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing values for feature 'title & BM25'\n",
      "Computing values for feature 'title & LM'\n",
      "Computing values for feature 'content & BM25'\n",
      "Computing values for feature 'content & LM'\n",
      "Computing values for feature 'anchors & BM25'\n",
      "Computing values for feature 'anchors & LM'\n"
     ]
    }
   ],
   "source": [
    "FIELD_MOD = {\n",
    "    1: [\"title\", \"BM25\"],\n",
    "    2: [\"title\", \"LM\"],\n",
    "    3: [\"content\", \"BM25\"],\n",
    "    4: [\"content\", \"LM\"],\n",
    "    5: [\"anchors\", \"BM25\"],\n",
    "    6: [\"anchors\", \"LM\"]\n",
    "}\n",
    "\n",
    "features = {}\n",
    "\n",
    "def add_to_features(docid):\n",
    "    if docid not in features[qid]:\n",
    "        features[qid][docid] = {}\n",
    "    features[qid][docid][fid] = r[\"_score\"]\n",
    "\n",
    "for fid in range(1, len(FIELD_MOD) + 1):\n",
    "    print(\"Computing values for feature '%s & %s'\" % (FIELD_MOD[fid][0], FIELD_MOD[fid][1]))\n",
    "\n",
    "    for qid, query in queries.items():\n",
    "        if qid not in features:\n",
    "            features[qid] = {}\n",
    "\n",
    "        if FIELD_MOD[fid][1] == \"BM25\":\n",
    "            if FIELD_MOD[fid][0] == \"anchors\":\n",
    "                res = search(\"clueweb12b_anchors\", query, FIELD_MOD[fid][0], size=20)\n",
    "                for r in res.get(\"hits\", {}).get(\"hits\", {}):\n",
    "                    docid = r[\"_id\"]\n",
    "                    if exists(\"clueweb12b\", r[\"_id\"])[\"exists\"] == True:\n",
    "                        add_to_features(docid)\n",
    "            else: \n",
    "                res = search(\"clueweb12b\", query, FIELD_MOD[fid][0], size=20)\n",
    "                for r in res.get(\"hits\", {}).get(\"hits\", {}):\n",
    "                    docid = r[\"_id\"]\n",
    "                    add_to_features(docid)\n",
    "            \n",
    "        else:\n",
    "            # TODO TODO TODO\n",
    "#             if FIELD_MOD[fid][0] == \"anchors\":\n",
    "#                 res = search(\"clueweb12b_anchors\", query, FIELD_MOD[fid][0], size=100)\n",
    "#             else: \n",
    "#                 res = search(\"clueweb12b\", query, FIELD_MOD[fid][0], size=100)\n",
    "#             # reranking must be implemented here\n",
    "#             qterms = analyze_query(\"clueweb12b\", query)\n",
    "            pass\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking up relevance labels and writing training data to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(FEATURES_FILE, \"w\") as fout:\n",
    "    for qid, query in queries.items():\n",
    "        for docid, ft in features[qid].items():\n",
    "            # Note that docid will not have a feature value for feature ID i\n",
    "            # if it was not retrieved in the top-1000 positions for that feature\n",
    "            # Here, we use -1 as the value for \"missing\" features\n",
    "            \n",
    "            # CHANGE range() PARAMETER WHEN READY\n",
    "            for fid in range(1, len(FIELD_MOD) + 1):\n",
    "                if fid not in ft:\n",
    "                    ft[fid] = -1\n",
    "            \n",
    "            # relevance label is determined based on the ground truth (qrels) file\n",
    "            label = 1 if docid in gtruth.get(qid, []) else 0\n",
    "                        \n",
    "            feat_str = ['{}:{}'.format(k,v) for k,v in ft.items()]\n",
    "            fout.write(\" \".join([str(label), qid, docid] + feat_str) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class for pointwise-based learning to rank model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PointWiseLTRModel(object):\n",
    "    def __init__(self, regressor):\n",
    "        \"\"\"\n",
    "        :param classifier: an instance of scikit-learn regressor\n",
    "        \"\"\"\n",
    "        self.regressor = regressor\n",
    "\n",
    "    def _train(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains and LTR model.\n",
    "        :param X: features of training instances\n",
    "        :param y: relevance assessments of training instances\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.regressor is not None\n",
    "        self.model = self.regressor.fit(X, y)\n",
    "\n",
    "    def rank(self, ft, doc_ids):\n",
    "        \"\"\"\n",
    "        Predicts relevance labels and rank documents for a given query\n",
    "        :param ft: a list of features for query-doc pairs\n",
    "        :param ft: a list of document ids\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        rel_labels = self.model.predict(ft)\n",
    "        sort_indices = np.argsort(rel_labels)[::-1]\n",
    "\n",
    "        results = []\n",
    "        for i in sort_indices:\n",
    "            results.append((doc_ids[i], rel_labels[i]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read training data from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data_from_file(path):\n",
    "    \"\"\"\n",
    "    :param path: path of file\n",
    "    :return: X features of data, y labels of data, group a list of numbers indicate how many instances for each query\n",
    "    \"\"\"\n",
    "    X, y, qids, doc_ids = [], [], [], []\n",
    "    with open(path, \"r\") as f:\n",
    "        i, s_qid = 0, None\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            label = int(items[0])\n",
    "            qid = items[1]\n",
    "            doc_id = items[2]\n",
    "            features = np.array([float(i.split(\":\")[1]) for i in items[3:]])\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "            qids.append(qid)\n",
    "            doc_ids.append(doc_id)\n",
    "\n",
    "    return X, y, qids, doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#queries:  50\n",
      "#query-doc pairs:  1912\n"
     ]
    }
   ],
   "source": [
    "X, y, qids, doc_ids = read_data_from_file(path=FEATURES_FILE)\n",
    "qids_unique= list(set(qids))\n",
    "\n",
    "print(\"#queries: \", len(qids_unique))\n",
    "print(\"#query-doc pairs: \", len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying 5-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 245\n",
      "\t\tRanking docs for queryID 221\n",
      "\t\tRanking docs for queryID 240\n",
      "\t\tRanking docs for queryID 246\n",
      "\t\tRanking docs for queryID 212\n",
      "\t\tRanking docs for queryID 213\n",
      "\t\tRanking docs for queryID 210\n",
      "\t\tRanking docs for queryID 248\n",
      "\t\tRanking docs for queryID 230\n",
      "\t\tRanking docs for queryID 203\n",
      "Fold #2\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 239\n",
      "\t\tRanking docs for queryID 231\n",
      "\t\tRanking docs for queryID 225\n",
      "\t\tRanking docs for queryID 202\n",
      "\t\tRanking docs for queryID 229\n",
      "\t\tRanking docs for queryID 234\n",
      "\t\tRanking docs for queryID 238\n",
      "\t\tRanking docs for queryID 216\n",
      "\t\tRanking docs for queryID 237\n",
      "\t\tRanking docs for queryID 214\n",
      "Fold #3\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 233\n",
      "\t\tRanking docs for queryID 215\n",
      "\t\tRanking docs for queryID 206\n",
      "\t\tRanking docs for queryID 224\n",
      "\t\tRanking docs for queryID 236\n",
      "\t\tRanking docs for queryID 227\n",
      "\t\tRanking docs for queryID 208\n",
      "\t\tRanking docs for queryID 247\n",
      "\t\tRanking docs for queryID 207\n",
      "\t\tRanking docs for queryID 219\n",
      "Fold #4\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 244\n",
      "\t\tRanking docs for queryID 205\n",
      "\t\tRanking docs for queryID 241\n",
      "\t\tRanking docs for queryID 250\n",
      "\t\tRanking docs for queryID 232\n",
      "\t\tRanking docs for queryID 249\n",
      "\t\tRanking docs for queryID 201\n",
      "\t\tRanking docs for queryID 220\n",
      "\t\tRanking docs for queryID 211\n",
      "\t\tRanking docs for queryID 223\n",
      "Fold #5\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 209\n",
      "\t\tRanking docs for queryID 228\n",
      "\t\tRanking docs for queryID 242\n",
      "\t\tRanking docs for queryID 243\n",
      "\t\tRanking docs for queryID 218\n",
      "\t\tRanking docs for queryID 222\n",
      "\t\tRanking docs for queryID 235\n",
      "\t\tRanking docs for queryID 217\n",
      "\t\tRanking docs for queryID 204\n",
      "\t\tRanking docs for queryID 226\n"
     ]
    }
   ],
   "source": [
    "FOLDS = 5\n",
    "\n",
    "fout = open(OUTPUT_FILE, \"w\")\n",
    "# write header\n",
    "fout.write(\"QueryId,DocumentId\\n\")\n",
    "    \n",
    "for f in range(FOLDS):\n",
    "    print(\"Fold #{}\".format(f + 1))\n",
    "    \n",
    "    train_qids, test_qids = [], []  # holds the IDs of train and test queries\n",
    "    train_ids, test_ids = [], []  # holds the instance IDs (indices in X )\n",
    "\n",
    "    for i in range(len(qids_unique)):\n",
    "        qid = qids_unique[i]\n",
    "        if i % FOLDS == f:  # test query\n",
    "            test_qids.append(qid)\n",
    "        else:  # train query\n",
    "            train_qids.append(qid)\n",
    "\n",
    "    train_X, train_y = [], []  # training feature values and target labels\n",
    "    test_X = []  # for testing we only have feature values\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if qids[i] in train_qids:\n",
    "            train_X.append(X[i])\n",
    "            train_y.append(y[i])\n",
    "        else:\n",
    "            test_X.append(X[i])\n",
    "\n",
    "    # Create and train LTR model\n",
    "    print(\"\\tTraining model ...\")\n",
    "    clf = RandomForestRegressor(max_depth=3, random_state=0)\n",
    "    ltr = PointWiseLTRModel(clf)\n",
    "    ltr._train(train_X, train_y)\n",
    "    \n",
    "    # Apply LTR model on the remaining fold (test queries)\n",
    "    print(\"\\tApplying model ...\")\n",
    "    \n",
    "    for qid in set(test_qids):\n",
    "        print(\"\\t\\tRanking docs for queryID {}\".format(qid))\n",
    "        # Collect the features and docids for that (test) query `qid`\n",
    "        test_ft, test_docids = [], []\n",
    "        for i in range(len(X)):\n",
    "            if qids[i] == qid:\n",
    "                test_ft.append(X[i])\n",
    "                test_docids.append(doc_ids[i])\n",
    "        \n",
    "        # Get ranking\n",
    "        r = ltr.rank(test_ft, test_docids)    \n",
    "        # Write the results to file\n",
    "        for doc, score in r:\n",
    "            fout.write(qid + \",\" + doc + \"\\n\")\n",
    "        \n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average (ltr):\n",
      "\tNDCG@10: 0.144 \n",
      "\tNDCG@20: 0.131 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dcg(rel, p):\n",
    "    dcg = rel[0]\n",
    "    for i in range(1, min(p, len(rel))): \n",
    "        dcg += rel[i] / math.log(i + 1, 2)  # rank position is indexed from 1..\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def evaluate(rankings, gtruth, df):\n",
    "    sum_ndcg10 = 0\n",
    "    sum_ndcg20 = 0\n",
    "    \n",
    "    for qid, ranking in sorted(rankings.items()):\n",
    "        gt = gtruth[qid]    \n",
    "\n",
    "        # relevance levels of our ranking\n",
    "        gains = []\n",
    "        for doc_id in ranking: \n",
    "            if gt.get(doc_id, 0) >= 0:\n",
    "                gains.append(gt.get(doc_id, 0))\n",
    "            else: \n",
    "                gains.append(0)\n",
    "        \n",
    "        # relevance levels of the idealized ranking\n",
    "        gain_ideal = sorted([v for _, v in gt.items()], reverse=True)\n",
    "\n",
    "        ndcg10 = dcg(gains, 10) / dcg(gain_ideal, 10)\n",
    "        ndcg20 = dcg(gains, 20) / dcg(gain_ideal, 20)\n",
    "        sum_ndcg10 += ndcg10\n",
    "        sum_ndcg20 += ndcg20\n",
    "\n",
    "        # print(\"NDCG@10:\", round(ndcg10, 3), \"\\nNDCG@20:\", round(ndcg20, 3))\n",
    "\n",
    "    print(\"\\nAverage (%s):\" % df)\n",
    "    print(\"\\tNDCG@10:\", round(sum_ndcg10 / len(rankings), 3), \"\\n\\tNDCG@20:\", round(sum_ndcg20 / len(rankings), 3), \"\\n\")\n",
    "    \n",
    "# load rankings for title field search\n",
    "rankings_ltr = {}\n",
    "with open(OUTPUT_FILE, \"r\") as fin:\n",
    "    docs = []\n",
    "    for line in fin.readlines():\n",
    "        if line.startswith('QueryId'):\n",
    "            continue\n",
    "        qid, doc_id = line.strip().split(\",\")\n",
    "        if qid not in rankings_ltr: \n",
    "            rankings_ltr[qid] = []\n",
    "        rankings_ltr[qid].append(doc_id)\n",
    "# evaluate\n",
    "evaluate(rankings_ltr, gtruth, \"ltr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
