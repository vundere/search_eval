{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Implement a learning-to-rank method with the following minimum requirements: \n",
    "        - Consider document-query matching in minimum 3 fields (title, content and anchors) \n",
    "          and at least two different retrieval models (e.g., BM25 and LM). That is, 6 document-query features minimum.\n",
    "\n",
    "Perform baseline (BM25) retrieval on a separate anchor text index. (THIS IS DONE IN 1_baseline.ipynb)\n",
    "        - The anchor text index (called clueweb12b_anchors) can be accessed the same way as the regular document index. \n",
    "        - Note that the anchor text index covers the entire ClueWeb collection, not just the Category B subset. \n",
    "          I.e., you need to ignore documents that are not present in the regular index.\n",
    "\n",
    "Test your model using 5-fold cross-validation on the given training data (queries and relevance judments, i.e., data/queries.txt and data/qrels.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "Load queries and qrels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "API = \"http://gustav1.ux.uis.no:5002\"\n",
    "\n",
    "QUERY_FILE = \"data/queries.txt\"\n",
    "QRELS_FILE = \"data/qrels.csv\"\n",
    "\n",
    "FEATURES_FILE = \"data/features_qd_q_d.txt\"\n",
    "OUTPUT_FILE = \"data/ltr_qd_q_d.txt\"  # output the ranking, only QD features\n",
    "\n",
    "FIELD_MOD = {\n",
    "    1: [\"title\", \"BM25\"],\n",
    "    2: [\"title\", \"LM\"],\n",
    "    3: [\"content\", \"BM25\"],\n",
    "    4: [\"content\", \"LM\"],\n",
    "    5: [\"anchors\", \"BM25\"],\n",
    "    6: [\"anchors\", \"LM\"]\n",
    "}\n",
    "\n",
    "BASIC_INDEX_NAME = \"clueweb12b\"\n",
    "ANCHORS_INDEX_NAME = \"clueweb12b_anchors\"\n",
    "\n",
    "CACHE_DIR = \"cache\"\n",
    "CACHE_DIR_SEARCH = CACHE_DIR + \"/search\"\n",
    "CACHE_DIR_TERMVECTORS = CACHE_DIR + \"/termvectors\"\n",
    "\n",
    "LAMBDA = 0.1\n",
    "\n",
    "def get_index_name(field):\n",
    "    return ANCHORS_INDEX_NAME if field == \"anchors\" else BASIC_INDEX_NAME\n",
    "\n",
    "# load queries\n",
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(QUERY_FILE, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries\n",
    "        \n",
    "# load given ground truth\n",
    "def load_qrels(qrels_file):\n",
    "    gtruth = {}\n",
    "    with open(QRELS_FILE, 'r') as qr:\n",
    "        for line in qr.readlines():\n",
    "            if line.startswith('QueryId'):\n",
    "                continue\n",
    "            qid, did, rel = line.strip().split(',')\n",
    "            if qid not in gtruth:\n",
    "                gtruth[qid] = {}\n",
    "            gtruth[qid][did] = int(rel)\n",
    "    return gtruth\n",
    "\n",
    "queries = load_queries(QUERY_FILE)\n",
    "gtruth = load_qrels(QRELS_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(indexname, query, field, size=10):\n",
    "    cache_file = CACHE_DIR_SEARCH + \"/\" + indexname + \"_\" + query + \"_\" + field + \"_\" + str(size)\n",
    "    url = \"/\".join([API, indexname, \"_search\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"q\": query, \"df\": field, \"size\": size})\n",
    "    if os.path.exists(cache_file):  # return from cache\n",
    "        with open(cache_file) as infile:\n",
    "            response = json.load(infile)\n",
    "            return json.loads(response)\n",
    "    else:\n",
    "        with open(cache_file, \"w\") as outfile:\n",
    "            response = requests.get(url).text\n",
    "            json.dump(response, outfile)\n",
    "            return json.loads(response)\n",
    "\n",
    "def termvectors(indexname, docid, term_statistics=\"true\"): \n",
    "    cache_file = CACHE_DIR_TERMVECTORS + \"/\" + indexname + \"_\" + docid + \"_\" + term_statistics\n",
    "    url = \"/\".join([API, indexname, docid, \"_termvectors\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"term_statistics\": term_statistics})\n",
    "    if os.path.exists(cache_file):  # return from cache\n",
    "        with open(cache_file) as infile:\n",
    "            response = json.load(infile)\n",
    "            return json.loads(response)\n",
    "    else:\n",
    "        with open(cache_file, \"w\") as outfile:\n",
    "            response = requests.get(url).text\n",
    "            json.dump(response, outfile)\n",
    "            return json.loads(response)\n",
    "\n",
    "def analyze_query(indexname, query):\n",
    "    url = \"/\".join([API, indexname, \"_analyze\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"text\": query})\n",
    "    response = json.loads(requests.get(url).text)\n",
    "    tokens = response[\"tokens\"]\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x[\"position\"]):\n",
    "        query_terms.append(t[\"token\"])\n",
    "    return query_terms\n",
    "\n",
    "def exists(indexname, docid): \n",
    "    url = \"/\".join([API, indexname, docid, \"_exists\"])\n",
    "    response = requests.get(url).text\n",
    "    return json.loads(response)\n",
    "\n",
    "\n",
    "# print(termvectors(\"clueweb12b\", \"clueweb12-0000tw-07-01629\").items())\n",
    "# print(analyze_query(\"clueweb12b\", \"raspberry pi\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "Collection LM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CollectionLM(object):\n",
    "    def __init__(self, qterms):\n",
    "        self._probs = {}\n",
    "        # computing P(t|C_i) for each field and for each query term\n",
    "        for fid in FIELD_MOD:\n",
    "            self._probs[FIELD_MOD[fid][0]] = {}\n",
    "            for t in qterms:\n",
    "                self._probs[FIELD_MOD[fid][0]][t] = self.__get_prob(FIELD_MOD[fid][0], t)\n",
    "\n",
    "    def __get_prob(self, field, term):\n",
    "        # use a boolean query to find a document that contains the term\n",
    "        index_name = get_index_name(field)\n",
    "        hits = search(index_name, term, field, size=1).get(\"hits\", {}).get(\"hits\", {})\n",
    "        doc_id = hits[0][\"_id\"] if len(hits) > 0 else None\n",
    "        if doc_id is not None:\n",
    "            # ask for global term statistics when requesting the term vector of that doc (`term_statistics=True` by default)\n",
    "            if termvectors(\"clueweb12b\", doc_id)[\"found\"] == True:\n",
    "                index_name = get_index_name(field)\n",
    "                tv = termvectors(index_name, doc_id)\n",
    "                ttf = tv[\"term_vectors\"][field][\"terms\"].get(term, {}).get(\"ttf\", 0)  # total term count in the collection (in that field)\n",
    "                sum_ttf = tv[\"term_vectors\"][field][\"field_statistics\"][\"sum_ttf\"]\n",
    "                return ttf / sum_ttf\n",
    "\n",
    "        return 0  # this only happens if none of the documents contain that term\n",
    "\n",
    "    def prob(self, field, term):\n",
    "        return self._probs.get(field, {}).get(term, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lm(clm, qterms, doc_id, field):\n",
    "    score = 0  # log P(q|d)\n",
    "    \n",
    "    # Getting term frequency statistics for the given document field from Elasticsearch\n",
    "    # Note that global term statistics are not needed\n",
    "    index_name = get_index_name(field)\n",
    "    tv = termvectors(index_name, doc_id, term_statistics=\"false\")[\"term_vectors\"]\n",
    "\n",
    "    # compute field length $|d|$\n",
    "    len_d = 0  # document field length initialization\n",
    "    if field in tv:  # that document field may be NOT empty\n",
    "        len_d = sum([s[\"term_freq\"] for t, s in tv[field][\"terms\"].items()])\n",
    "        \n",
    "    # scoring the query\n",
    "    for t in qterms:\n",
    "        Pt_theta_d = 0  # P(t|\\theta_d)\n",
    "        if field in tv:\n",
    "            Pt_d = tv[field][\"terms\"].get(t, {}).get(\"term_freq\", 0) / len_d  # $P(t|d)$\n",
    "        else:  # that document field is empty\n",
    "            Pt_d = 0\n",
    "        Pt_C = clm.prob(field, t)  # $P(t|C)$\n",
    "        Pt_theta_d = (1 - LAMBDA) * Pt_d + LAMBDA * Pt_C  # $P(t|\\theta_{d})$ with J-M smoothing\n",
    "        # Pt_theta_d is 0 if t doesn't occur in any doc for that field, even with smoothing:\n",
    "        score += math.log(Pt_theta_d) if Pt_theta_d > 0 else 0  \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "Collecting feature values in the features dict. It has the structure features[qid][docid][fid] = value, where fid is a feature ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing values for feature 'title & BM25' . . .\n",
      "Query 'raspberry pi'\n",
      "Query 'uss carl vinson'\n",
      "Query 'reviews of les miserables'\n",
      "Query 'rules of golf'\n",
      "Query 'average charitable donation'\n",
      "Query 'wind power'\n",
      "Query 'bph treatment'\n",
      "Query 'doctor zhivago'\n",
      "Query 'land surveyor'\n",
      "Query 'golf gps'\n",
      "Query 'what is madagascar known for'\n",
      "Query 'home theater systems'\n",
      "Query 'carpal tunnel syndrome'\n",
      "Query 'capital gains tax rate'\n",
      "Query 'maryland department of natural resources'\n",
      "Query 'nicolas cage movies'\n",
      "Query 'kids earth day activities'\n",
      "Query 'solar water fountains'\n",
      "Query 'what was the name of elvis presley's home'\n",
      "Query 'nba records'\n",
      "Query 'electoral college 2008 results'\n",
      "Query 'male menopause'\n",
      "Query 'usda food pyramid'\n",
      "Query 'making chicken soup from scratch'\n",
      "Query 'black and gold'\n",
      "Query 'traverse city'\n",
      "Query 'i will survive lyrics'\n",
      "Query 'hawaiian volcano observatories'\n",
      "Query 'beef stroganoff recipe'\n",
      "Query 'world's biggest dog'\n",
      "Query 'what are the seven deadly sins'\n",
      "Query 'hurricane Irene flooding in manville nj'\n",
      "Query 'hair dye'\n",
      "Query 'dark chocolate health benefits'\n",
      "Query 'ham radio'\n",
      "Query 'symptoms of mad cow disease in humans'\n",
      "Query 'lump in throat'\n",
      "Query 'george bush sr bio'\n",
      "Query 'frank lloyd wright biography'\n",
      "Query 'presidential middle names'\n",
      "Query 'what is a wiki'\n",
      "Query 'cannellini beans'\n",
      "Query 'afghanistan flag'\n",
      "Query 'old town scottsdale'\n",
      "Query 'roosevelt island'\n",
      "Query 'civil war battles in South Carolina'\n",
      "Query 'rain man'\n",
      "Query 'eggs shelf life'\n",
      "Query 'occupational therapist'\n",
      "Query 'ford edge problems'\n",
      "\n",
      "Computing values for feature 'title & LM' . . .\n",
      "Query 'raspberry pi'\n",
      "Query 'uss carl vinson'\n",
      "Query 'reviews of les miserables'\n",
      "Query 'rules of golf'\n",
      "Query 'average charitable donation'\n",
      "Query 'wind power'\n",
      "Query 'bph treatment'\n",
      "Query 'doctor zhivago'\n",
      "Query 'land surveyor'\n",
      "Query 'golf gps'\n",
      "Query 'what is madagascar known for'\n",
      "Query 'home theater systems'\n",
      "Query 'carpal tunnel syndrome'\n",
      "Query 'capital gains tax rate'\n",
      "Query 'maryland department of natural resources'\n",
      "Query 'nicolas cage movies'\n",
      "Query 'kids earth day activities'\n",
      "Query 'solar water fountains'\n",
      "Query 'what was the name of elvis presley's home'\n",
      "Query 'nba records'\n",
      "Query 'electoral college 2008 results'\n",
      "Query 'male menopause'\n",
      "Query 'usda food pyramid'\n",
      "Query 'making chicken soup from scratch'\n",
      "Query 'black and gold'\n",
      "Query 'traverse city'\n",
      "Query 'i will survive lyrics'\n",
      "Query 'hawaiian volcano observatories'\n",
      "Query 'beef stroganoff recipe'\n",
      "Query 'world's biggest dog'\n",
      "Query 'what are the seven deadly sins'\n",
      "Query 'hurricane Irene flooding in manville nj'\n",
      "Query 'hair dye'\n",
      "Query 'dark chocolate health benefits'\n",
      "Query 'ham radio'\n",
      "Query 'symptoms of mad cow disease in humans'\n",
      "Query 'lump in throat'\n",
      "Query 'george bush sr bio'\n",
      "Query 'frank lloyd wright biography'\n",
      "Query 'presidential middle names'\n",
      "Query 'what is a wiki'\n",
      "Query 'cannellini beans'\n",
      "Query 'afghanistan flag'\n",
      "Query 'old town scottsdale'\n",
      "Query 'roosevelt island'\n",
      "Query 'civil war battles in South Carolina'\n",
      "Query 'rain man'\n",
      "Query 'eggs shelf life'\n",
      "Query 'occupational therapist'\n",
      "Query 'ford edge problems'\n",
      "\n",
      "Computing values for feature 'content & BM25' . . .\n",
      "Query 'raspberry pi'\n",
      "Query 'uss carl vinson'\n",
      "Query 'reviews of les miserables'\n",
      "Query 'rules of golf'\n",
      "Query 'average charitable donation'\n",
      "Query 'wind power'\n",
      "Query 'bph treatment'\n",
      "Query 'doctor zhivago'\n",
      "Query 'land surveyor'\n",
      "Query 'golf gps'\n",
      "Query 'what is madagascar known for'\n",
      "Query 'home theater systems'\n",
      "Query 'carpal tunnel syndrome'\n",
      "Query 'capital gains tax rate'\n",
      "Query 'maryland department of natural resources'\n",
      "Query 'nicolas cage movies'\n",
      "Query 'kids earth day activities'\n",
      "Query 'solar water fountains'\n",
      "Query 'what was the name of elvis presley's home'\n",
      "Query 'nba records'\n",
      "Query 'electoral college 2008 results'\n",
      "Query 'male menopause'\n",
      "Query 'usda food pyramid'\n",
      "Query 'making chicken soup from scratch'\n",
      "Query 'black and gold'\n",
      "Query 'traverse city'\n",
      "Query 'i will survive lyrics'\n",
      "Query 'hawaiian volcano observatories'\n",
      "Query 'beef stroganoff recipe'\n",
      "Query 'world's biggest dog'\n",
      "Query 'what are the seven deadly sins'\n",
      "Query 'hurricane Irene flooding in manville nj'\n",
      "Query 'hair dye'\n",
      "Query 'dark chocolate health benefits'\n",
      "Query 'ham radio'\n",
      "Query 'symptoms of mad cow disease in humans'\n",
      "Query 'lump in throat'\n",
      "Query 'george bush sr bio'\n",
      "Query 'frank lloyd wright biography'\n",
      "Query 'presidential middle names'\n",
      "Query 'what is a wiki'\n",
      "Query 'cannellini beans'\n",
      "Query 'afghanistan flag'\n",
      "Query 'old town scottsdale'\n",
      "Query 'roosevelt island'\n",
      "Query 'civil war battles in South Carolina'\n",
      "Query 'rain man'\n",
      "Query 'eggs shelf life'\n",
      "Query 'occupational therapist'\n",
      "Query 'ford edge problems'\n",
      "\n",
      "Computing values for feature 'content & LM' . . .\n",
      "Query 'raspberry pi'\n",
      "Query 'uss carl vinson'\n",
      "Query 'reviews of les miserables'\n",
      "Query 'rules of golf'\n",
      "Query 'average charitable donation'\n",
      "Query 'wind power'\n",
      "Query 'bph treatment'\n",
      "Query 'doctor zhivago'\n",
      "Query 'land surveyor'\n",
      "Query 'golf gps'\n",
      "Query 'what is madagascar known for'\n",
      "Query 'home theater systems'\n",
      "Query 'carpal tunnel syndrome'\n",
      "Query 'capital gains tax rate'\n",
      "Query 'maryland department of natural resources'\n",
      "Query 'nicolas cage movies'\n",
      "Query 'kids earth day activities'\n",
      "Query 'solar water fountains'\n",
      "Query 'what was the name of elvis presley's home'\n",
      "Query 'nba records'\n",
      "Query 'electoral college 2008 results'\n",
      "Query 'male menopause'\n",
      "Query 'usda food pyramid'\n",
      "Query 'making chicken soup from scratch'\n",
      "Query 'black and gold'\n",
      "Query 'traverse city'\n",
      "Query 'i will survive lyrics'\n",
      "Query 'hawaiian volcano observatories'\n",
      "Query 'beef stroganoff recipe'\n",
      "Query 'world's biggest dog'\n",
      "Query 'what are the seven deadly sins'\n",
      "Query 'hurricane Irene flooding in manville nj'\n",
      "Query 'hair dye'\n",
      "Query 'dark chocolate health benefits'\n",
      "Query 'ham radio'\n",
      "Query 'symptoms of mad cow disease in humans'\n",
      "Query 'lump in throat'\n",
      "Query 'george bush sr bio'\n",
      "Query 'frank lloyd wright biography'\n",
      "Query 'presidential middle names'\n",
      "Query 'what is a wiki'\n",
      "Query 'cannellini beans'\n",
      "Query 'afghanistan flag'\n",
      "Query 'old town scottsdale'\n",
      "Query 'roosevelt island'\n",
      "Query 'civil war battles in South Carolina'\n",
      "Query 'rain man'\n",
      "Query 'eggs shelf life'\n",
      "Query 'occupational therapist'\n",
      "Query 'ford edge problems'\n",
      "\n",
      "Computing values for feature 'anchors & BM25' . . .\n",
      "Query 'raspberry pi'\n",
      "Query 'uss carl vinson'\n",
      "Query 'reviews of les miserables'\n",
      "Query 'rules of golf'\n",
      "Query 'average charitable donation'\n",
      "Query 'wind power'\n",
      "Query 'bph treatment'\n",
      "Query 'doctor zhivago'\n",
      "Query 'land surveyor'\n",
      "Query 'golf gps'\n",
      "Query 'what is madagascar known for'\n",
      "Query 'home theater systems'\n",
      "Query 'carpal tunnel syndrome'\n",
      "Query 'capital gains tax rate'\n",
      "Query 'maryland department of natural resources'\n",
      "Query 'nicolas cage movies'\n",
      "Query 'kids earth day activities'\n",
      "Query 'solar water fountains'\n",
      "Query 'what was the name of elvis presley's home'\n",
      "Query 'nba records'\n",
      "Query 'electoral college 2008 results'\n",
      "Query 'male menopause'\n",
      "Query 'usda food pyramid'\n",
      "Query 'making chicken soup from scratch'\n",
      "Query 'black and gold'\n",
      "Query 'traverse city'\n",
      "Query 'i will survive lyrics'\n",
      "Query 'hawaiian volcano observatories'\n",
      "Query 'beef stroganoff recipe'\n",
      "Query 'world's biggest dog'\n",
      "Query 'what are the seven deadly sins'\n",
      "Query 'hurricane Irene flooding in manville nj'\n",
      "Query 'hair dye'\n",
      "Query 'dark chocolate health benefits'\n",
      "Query 'ham radio'\n",
      "Query 'symptoms of mad cow disease in humans'\n",
      "Query 'lump in throat'\n",
      "Query 'george bush sr bio'\n",
      "Query 'frank lloyd wright biography'\n",
      "Query 'presidential middle names'\n",
      "Query 'what is a wiki'\n",
      "Query 'cannellini beans'\n",
      "Query 'afghanistan flag'\n",
      "Query 'old town scottsdale'\n",
      "Query 'roosevelt island'\n",
      "Query 'civil war battles in South Carolina'\n",
      "Query 'rain man'\n",
      "Query 'eggs shelf life'\n",
      "Query 'occupational therapist'\n",
      "Query 'ford edge problems'\n",
      "\n",
      "Computing values for feature 'anchors & LM' . . .\n",
      "Query 'raspberry pi'\n",
      "Query 'uss carl vinson'\n",
      "Query 'reviews of les miserables'\n",
      "Query 'rules of golf'\n",
      "Query 'average charitable donation'\n",
      "Query 'wind power'\n",
      "Query 'bph treatment'\n",
      "Query 'doctor zhivago'\n",
      "Query 'land surveyor'\n",
      "Query 'golf gps'\n",
      "Query 'what is madagascar known for'\n",
      "Query 'home theater systems'\n",
      "Query 'carpal tunnel syndrome'\n",
      "Query 'capital gains tax rate'\n",
      "Query 'maryland department of natural resources'\n",
      "Query 'nicolas cage movies'\n",
      "Query 'kids earth day activities'\n",
      "Query 'solar water fountains'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 'what was the name of elvis presley's home'\n",
      "Query 'nba records'\n",
      "Query 'electoral college 2008 results'\n",
      "Query 'male menopause'\n",
      "Query 'usda food pyramid'\n",
      "Query 'making chicken soup from scratch'\n",
      "Query 'black and gold'\n",
      "Query 'traverse city'\n",
      "Query 'i will survive lyrics'\n",
      "Query 'hawaiian volcano observatories'\n",
      "Query 'beef stroganoff recipe'\n",
      "Query 'world's biggest dog'\n",
      "Query 'what are the seven deadly sins'\n",
      "Query 'hurricane Irene flooding in manville nj'\n",
      "Query 'hair dye'\n",
      "Query 'dark chocolate health benefits'\n",
      "Query 'ham radio'\n",
      "Query 'symptoms of mad cow disease in humans'\n",
      "Query 'lump in throat'\n",
      "Query 'george bush sr bio'\n",
      "Query 'frank lloyd wright biography'\n",
      "Query 'presidential middle names'\n",
      "Query 'what is a wiki'\n",
      "Query 'cannellini beans'\n",
      "Query 'afghanistan flag'\n",
      "Query 'old town scottsdale'\n",
      "Query 'roosevelt island'\n",
      "Query 'civil war battles in South Carolina'\n",
      "Query 'rain man'\n",
      "Query 'eggs shelf life'\n",
      "Query 'occupational therapist'\n",
      "Query 'ford edge problems'\n",
      "Computing values for additional query features . . .\n",
      "Computing values for additional document features . . .\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "\n",
    "def add_to_features(docid):\n",
    "    if docid not in features[qid]:\n",
    "        features[qid][docid] = {}\n",
    "    features[qid][docid][fid] = r[\"_score\"]\n",
    "\n",
    "for fid in range(1, len(FIELD_MOD) + 1):\n",
    "    print(\"\\nComputing values for feature '%s & %s' . . .\" % (FIELD_MOD[fid][0], FIELD_MOD[fid][1]))\n",
    "\n",
    "    for qid, query in queries.items():\n",
    "        if qid not in features:\n",
    "            features[qid] = {}\n",
    "\n",
    "        if FIELD_MOD[fid][1] == \"BM25\":\n",
    "            print(\"Query '{0}'\".format(query))\n",
    "            if FIELD_MOD[fid][0] == \"anchors\":\n",
    "                res = search(\"clueweb12b_anchors\", query, FIELD_MOD[fid][0], size=20)\n",
    "                for r in res[\"hits\"][\"hits\"]:\n",
    "                    docid = r[\"_id\"]\n",
    "                    if exists(\"clueweb12b\", r[\"_id\"])[\"exists\"] == True:\n",
    "                        add_to_features(docid)\n",
    "            else: \n",
    "                res = search(\"clueweb12b\", query, FIELD_MOD[fid][0], size=20)                \n",
    "                for r in res[\"hits\"][\"hits\"]:\n",
    "                    docid = r[\"_id\"]\n",
    "                    add_to_features(docid)\n",
    "        else:  \n",
    "            # LM\n",
    "            print(\"Query '{0}'\".format(query))\n",
    "            if FIELD_MOD[fid][0] == \"anchors\":\n",
    "                res = search(\"clueweb12b_anchors\", query, FIELD_MOD[fid][0], size=100)\n",
    "                for r in res[\"hits\"][\"hits\"]:\n",
    "                    docid = r[\"_id\"]\n",
    "                    if exists(\"clueweb12b\", r[\"_id\"])[\"exists\"] == True:\n",
    "                        qterms = analyze_query(\"clueweb12b\", query)\n",
    "                        clm = CollectionLM(qterms)\n",
    "                        scores = {}\n",
    "                        for r in res[\"hits\"][\"hits\"]:\n",
    "                            docid = r[\"_id\"]\n",
    "                            scores[docid] = lm(clm, qterms, docid, FIELD_MOD[fid][0])\n",
    "                        i = 1\n",
    "                        for docid in scores:\n",
    "                            # return top 20\n",
    "                            if i > 20:\n",
    "                                break\n",
    "                            if docid not in features[qid]:\n",
    "                                features[qid][docid] = {}\n",
    "                            features[qid][docid][fid] = scores[docid]\n",
    "                            i += 1\n",
    "            else:\n",
    "                res = search(\"clueweb12b\", query, FIELD_MOD[fid][0], size=100)  # size=100 because of later re-ranking\n",
    "                qterms = analyze_query(\"clueweb12b\", query)\n",
    "                clm = CollectionLM(qterms)\n",
    "                scores = {}\n",
    "                for r in res[\"hits\"][\"hits\"]:\n",
    "                    docid = r[\"_id\"]\n",
    "                    scores[docid] = lm(clm, qterms, docid, FIELD_MOD[fid][0])\n",
    "                i = 1\n",
    "                for docid in scores:\n",
    "                    # return top 20\n",
    "                    if i > 20:\n",
    "                        break\n",
    "                    if docid not in features[qid]:\n",
    "                        features[qid][docid] = {}\n",
    "                    features[qid][docid][fid] = scores[docid]\n",
    "                    i += 1\n",
    "\n",
    "# additional query features\n",
    "print(\"Computing values for additional query features . . .\")\n",
    "for qid, query in queries.items():\n",
    "    avg_arr = []\n",
    "    for e in features[qid]:\n",
    "        if 1 in features[qid][e].keys():\n",
    "            avg_arr.append(features[qid][e][1])\n",
    "    \n",
    "    for entry in features[qid]:\n",
    "        ql = len(query.split(' '))\n",
    "        # hardcoded feature IDs\n",
    "        features[qid][entry][7] = ql\n",
    "        features[qid][entry][8] = sum(avg_arr) / len(avg_arr)\n",
    "\n",
    "# additional document features\n",
    "print(\"Computing values for additional document features . . .\")\n",
    "for qid in queries.keys():\n",
    "    for doc_id in features[qid]:\n",
    "        for field in ['title', 'content']:\n",
    "            index_name = get_index_name(field)\n",
    "            try:\n",
    "                tv = termvectors(index_name, doc_id, term_statistics=\"false\")[\"term_vectors\"]\n",
    "                if field in tv:\n",
    "                    len_d = sum([s[\"term_freq\"] for t, s in tv[field][\"terms\"].items()])\n",
    "                    if field == 'title':\n",
    "                        features[qid][doc_id][9] = len_d\n",
    "                    else:\n",
    "                        features[qid][doc_id][10] = len_d\n",
    "            except KeyError:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking up relevance labels and writing training data to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(FEATURES_FILE, \"w\") as fout:\n",
    "    for qid, query in queries.items():\n",
    "        for docid, ft in features[qid].items():\n",
    "            # Note that docid will not have a feature value for feature ID i\n",
    "            # if it was not retrieved in the top-1000 positions for that feature\n",
    "            # Here, we use -1 as the value for \"missing\" features\n",
    "            \n",
    "            # CHANGE range() PARAMETER WHEN READY\n",
    "            for fid in range(1, 11):\n",
    "                if fid not in ft:\n",
    "                    ft[fid] = -1\n",
    "            \n",
    "            # relevance label is determined based on the ground truth (qrels) file\n",
    "            label = 1 if docid in gtruth.get(qid, []) else 0\n",
    "                        \n",
    "            feat_str = ['{}:{}'.format(k,v) for k,v in ft.items()]\n",
    "            fout.write(\" \".join([str(label), qid, docid] + feat_str) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A class for pointwise-based learning to rank model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PointWiseLTRModel(object):\n",
    "    def __init__(self, regressor):\n",
    "        \"\"\"\n",
    "        :param classifier: an instance of scikit-learn regressor\n",
    "        \"\"\"\n",
    "        self.regressor = regressor\n",
    "\n",
    "    def _train(self, X, y):\n",
    "        \"\"\"\n",
    "        Trains and LTR model.\n",
    "        :param X: features of training instances\n",
    "        :param y: relevance assessments of training instances\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.regressor is not None\n",
    "        self.model = self.regressor.fit(X, y)\n",
    "\n",
    "    def rank(self, ft, doc_ids):\n",
    "        \"\"\"\n",
    "        Predicts relevance labels and rank documents for a given query\n",
    "        :param ft: a list of features for query-doc pairs\n",
    "        :param ft: a list of document ids\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self.model is not None\n",
    "        rel_labels = self.model.predict(ft)\n",
    "        sort_indices = np.argsort(rel_labels)[::-1]\n",
    "\n",
    "        results = []\n",
    "        for i in sort_indices:\n",
    "            results.append((doc_ids[i], rel_labels[i]))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read training data from file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data_from_file(path):\n",
    "    \"\"\"\n",
    "    :param path: path of file\n",
    "    :return: X features of data, y labels of data, group a list of numbers indicate how many instances for each query\n",
    "    \"\"\"\n",
    "    X, y, qids, doc_ids = [], [], [], []\n",
    "    with open(path, \"r\") as f:\n",
    "        i, s_qid = 0, None\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            label = int(items[0])\n",
    "            qid = items[1]\n",
    "            doc_id = items[2]\n",
    "            features = np.array([float(i.split(\":\")[1]) for i in items[3:]])\n",
    "            X.append(features)\n",
    "            y.append(label)\n",
    "            qids.append(qid)\n",
    "            doc_ids.append(doc_id)\n",
    "\n",
    "    return X, y, qids, doc_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#queries:  50\n",
      "#query-doc pairs:  2865\n"
     ]
    }
   ],
   "source": [
    "X, y, qids, doc_ids = read_data_from_file(path=FEATURES_FILE)\n",
    "qids_unique= list(set(qids))\n",
    "\n",
    "print(\"#queries: \", len(qids_unique))\n",
    "print(\"#query-doc pairs: \", len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying 5-fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 237\n",
      "\t\tRanking docs for queryID 230\n",
      "\t\tRanking docs for queryID 245\n",
      "\t\tRanking docs for queryID 216\n",
      "\t\tRanking docs for queryID 213\n",
      "\t\tRanking docs for queryID 201\n",
      "\t\tRanking docs for queryID 225\n",
      "\t\tRanking docs for queryID 220\n",
      "\t\tRanking docs for queryID 218\n",
      "\t\tRanking docs for queryID 236\n",
      "Fold #2\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 242\n",
      "\t\tRanking docs for queryID 211\n",
      "\t\tRanking docs for queryID 222\n",
      "\t\tRanking docs for queryID 247\n",
      "\t\tRanking docs for queryID 234\n",
      "\t\tRanking docs for queryID 238\n",
      "\t\tRanking docs for queryID 210\n",
      "\t\tRanking docs for queryID 250\n",
      "\t\tRanking docs for queryID 224\n",
      "\t\tRanking docs for queryID 226\n",
      "Fold #3\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 248\n",
      "\t\tRanking docs for queryID 231\n",
      "\t\tRanking docs for queryID 239\n",
      "\t\tRanking docs for queryID 249\n",
      "\t\tRanking docs for queryID 228\n",
      "\t\tRanking docs for queryID 205\n",
      "\t\tRanking docs for queryID 206\n",
      "\t\tRanking docs for queryID 214\n",
      "\t\tRanking docs for queryID 219\n",
      "\t\tRanking docs for queryID 241\n",
      "Fold #4\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 240\n",
      "\t\tRanking docs for queryID 246\n",
      "\t\tRanking docs for queryID 235\n",
      "\t\tRanking docs for queryID 229\n",
      "\t\tRanking docs for queryID 202\n",
      "\t\tRanking docs for queryID 217\n",
      "\t\tRanking docs for queryID 223\n",
      "\t\tRanking docs for queryID 221\n",
      "\t\tRanking docs for queryID 244\n",
      "\t\tRanking docs for queryID 215\n",
      "Fold #5\n",
      "\tTraining model ...\n",
      "\tApplying model ...\n",
      "\t\tRanking docs for queryID 232\n",
      "\t\tRanking docs for queryID 227\n",
      "\t\tRanking docs for queryID 204\n",
      "\t\tRanking docs for queryID 243\n",
      "\t\tRanking docs for queryID 203\n",
      "\t\tRanking docs for queryID 233\n",
      "\t\tRanking docs for queryID 212\n",
      "\t\tRanking docs for queryID 209\n",
      "\t\tRanking docs for queryID 208\n",
      "\t\tRanking docs for queryID 207\n"
     ]
    }
   ],
   "source": [
    "FOLDS = 5\n",
    "\n",
    "fout = open(OUTPUT_FILE, \"w\")\n",
    "# write header\n",
    "fout.write(\"QueryId,DocumentId\\n\")\n",
    "    \n",
    "for f in range(FOLDS):\n",
    "    print(\"Fold #{}\".format(f + 1))\n",
    "    \n",
    "    train_qids, test_qids = [], []  # holds the IDs of train and test queries\n",
    "    train_ids, test_ids = [], []  # holds the instance IDs (indices in X )\n",
    "\n",
    "    for i in range(len(qids_unique)):\n",
    "        qid = qids_unique[i]\n",
    "        if i % FOLDS == f:  # test query\n",
    "            test_qids.append(qid)\n",
    "        else:  # train query\n",
    "            train_qids.append(qid)\n",
    "\n",
    "    train_X, train_y = [], []  # training feature values and target labels\n",
    "    test_X = []  # for testing we only have feature values\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        if qids[i] in train_qids:\n",
    "            train_X.append(X[i])\n",
    "            train_y.append(y[i])\n",
    "        else:\n",
    "            test_X.append(X[i])\n",
    "\n",
    "    # Create and train LTR model\n",
    "    print(\"\\tTraining model ...\")\n",
    "    clf = RandomForestRegressor(max_depth=3, random_state=0)\n",
    "    ltr = PointWiseLTRModel(clf)\n",
    "    ltr._train(train_X, train_y)\n",
    "    \n",
    "    # Apply LTR model on the remaining fold (test queries)\n",
    "    print(\"\\tApplying model ...\")\n",
    "    \n",
    "    for qid in set(test_qids):\n",
    "        print(\"\\t\\tRanking docs for queryID {}\".format(qid))\n",
    "        # Collect the features and docids for that (test) query `qid`\n",
    "        test_ft, test_docids = [], []\n",
    "        for i in range(len(X)):\n",
    "            if qids[i] == qid:\n",
    "                test_ft.append(X[i])\n",
    "                test_docids.append(doc_ids[i])\n",
    "        \n",
    "        # Get ranking\n",
    "        r = ltr.rank(test_ft, test_docids)    \n",
    "        # Write the results to file\n",
    "        for doc, score in r:\n",
    "            fout.write(qid + \",\" + doc + \"\\n\")\n",
    "        \n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "Evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average (LTR QD+Q+D):\n",
      "\tNDCG@10: 0.151 \n",
      "\tNDCG@20: 0.147 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def dcg(rel, p):\n",
    "    dcg = rel[0]\n",
    "    for i in range(1, min(p, len(rel))): \n",
    "        dcg += rel[i] / math.log(i + 1, 2)  # rank position is indexed from 1..\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def evaluate(rankings, gtruth, df):\n",
    "    sum_ndcg10 = 0\n",
    "    sum_ndcg20 = 0\n",
    "    \n",
    "    for qid, ranking in sorted(rankings.items()):\n",
    "        gt = gtruth[qid]    \n",
    "\n",
    "        # relevance levels of our ranking\n",
    "        gains = []\n",
    "        for doc_id in ranking: \n",
    "            if gt.get(doc_id, 0) >= 0:\n",
    "                gains.append(gt.get(doc_id, 0))\n",
    "            else: \n",
    "                gains.append(0)\n",
    "        \n",
    "        # relevance levels of the idealized ranking\n",
    "        gain_ideal = sorted([v for _, v in gt.items()], reverse=True)\n",
    "\n",
    "        ndcg10 = dcg(gains, 10) / dcg(gain_ideal, 10)\n",
    "        ndcg20 = dcg(gains, 20) / dcg(gain_ideal, 20)\n",
    "        sum_ndcg10 += ndcg10\n",
    "        sum_ndcg20 += ndcg20\n",
    "\n",
    "        # print(\"NDCG@10:\", round(ndcg10, 3), \"\\nNDCG@20:\", round(ndcg20, 3))\n",
    "\n",
    "    print(\"\\nAverage (%s):\" % df)\n",
    "    print(\"\\tNDCG@10:\", round(sum_ndcg10 / len(rankings), 3), \"\\n\\tNDCG@20:\", round(sum_ndcg20 / len(rankings), 3), \"\\n\")\n",
    "    \n",
    "# load rankings for LTR QD+Q+D\n",
    "rankings_ltr = {}\n",
    "with open(OUTPUT_FILE, \"r\") as fin:\n",
    "    docs = []\n",
    "    for line in fin.readlines():\n",
    "        if line.startswith('QueryId'):\n",
    "            continue\n",
    "        qid, doc_id = line.strip().split(\",\")\n",
    "        if qid not in rankings_ltr: \n",
    "            rankings_ltr[qid] = []\n",
    "        rankings_ltr[qid].append(doc_id)\n",
    "# evaluate\n",
    "evaluate(rankings_ltr, gtruth, \"LTR QD+Q+D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
